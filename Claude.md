# Claude.md - 产品设计哲学

## 一切皆对话

Attention OS 的核心设计哲学是：**一切皆对话**。

我们相信未来的应用程序应该从根本上围绕**对话框**而非传统的按钮、表单或菜单来构建。这意味着：

### 设计原则

1. **对话驱动的交互**
   - 用户与系统的每一次交互都应该是一场自然、流畅的对话
   - 不是填表单，而是讲述你的意图，让系统理解并响应
   - 例如："我想在明天下午完成这个项目"，而不是手动选择日期、时间、优先级

2. **智能对话框而非传统 UI 控件**
   - 用自然语言处理替代复杂的选项面板
   - 每个决定点都可以通过对话完成
   - 系统根据上下文理解用户意图，而不需要明确指示

3. **对话记录即数据**
   - 所有对话都是可追溯的历史记录
   - 用户可以通过回顾对话来理解自己的决策过程
   - 对话成为系统的第一类数据

4. **连续的对话流**
   - 从早间 Briefing（"今天想做什么？"）
   - 到专注时段的提醒（"还剩 12 分钟，坚持住！"）
   - 再到晚间回顾（"让我们一起回顾今天"）
   - 整天形成一条完整的对话线

### 实现路线

- **V5.2 现状**：多轮文本对话、LLM 驱动的理解
- **V5.3+**：集成更多对话通道（语音、多模态输入）
- **未来**：将所有参数配置改为对话交互，消除传统设置面板

---

## 测试指南

为了确保 Attention OS 的稳定性和可靠性，我们重视测试。

### 测试策略

```
├── 单元测试 (Unit Tests)
│   ├── agents.py - Multi-Agent 逻辑
│   ├── llm_client.py - LLM 调用层
│   ├── todo_manager.py - 任务解析
│   └── state_fusion.py - 多信号融合
│
├── 集成测试 (Integration Tests)
│   ├── 端到端工作流 (E2E)
│   ├── Web API 端点
│   ├── 数据库操作
│   └── LLM 调用链路
│
└── 系统测试 (System Tests)
    ├── 多平台兼容性
    ├── 性能基准测试
    └── 稳定性测试（长时间运行）
```

### 运行测试

```bash
# 运行所有测试
pytest tests/ -v

# 运行特定模块测试
pytest tests/test_agents.py -v

# 生成覆盖率报告
pytest tests/ --cov=. --cov-report=html
```

### 贡献代码时的测试要求

- ✅ 新功能必须包含相应的单元测试
- ✅ 修复 Bug 应包含测试用例（防止回归）
- ✅ 集成测试覆盖所有 API 端点
- ✅ 保持测试覆盖率 ≥ 80%

---

## 开发指导

### 添加新 Agent

当你想添加新的 AI Agent 时，遵循这个流程：

1. 在 `agents.py` 中定义 Agent 角色和提示词
2. 在 `llm_client.py` 中添加调用方法
3. 为新 Agent 编写单元测试
4. 在主业务逻辑中集成
5. 更新 README.md 中的架构表

### 对话框改进建议

- [ ] 将所有设置面板转换为对话式配置
- [ ] 添加多轮对话记忆（Conversation Context）
- [ ] 支持用户自定义对话流程
- [ ] 实现对话历史的搜索和回溯

---

## 参考资源

- [Claude API 文档](https://docs.anthropic.com/)
- [Multi-Agent 系统设计](https://www.anthropic.com/)
- 本项目的 Agent 实现：`agents.py`
